{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Recall, the model for linear regression:\n",
        "\n",
        "\\begin{align}\n",
        "\\vec{y} = f(\\vec{w}) = \\sum_{i=1}^{m} w_i x_i\n",
        "\\end{align}\n",
        "\n",
        "and the task is to estime the least squares error (LSE) regression coeffficient $w_i$ for predicting the output $\\vec{y}$ based on the observed data $X$, using gradient descent.\n",
        "\n",
        "\n",
        "1. Normalize the features in the training data using $z$-score normalization.\n",
        "\n",
        "2. Initialize the weights for the gradient descent algorithm all zeros i.e.\n",
        "\\begin{align}\n",
        "\\vec{w} = \n",
        "\\begin{bmatrix}\n",
        "0\\\\\n",
        "0\\\\\n",
        "0\\\\\n",
        "\\vdots\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "3. Using the following data set of parameters:\n",
        "\n",
        "  * **Housing**: learning rate $= 0.4 \\times 10^{-3}$, tolerance $= 0.5 \\times 10^{-2}$.\n",
        "\n",
        "  * **Yacht**: learning rate $= 0.1 \\times 10^{-2}$, tolaerance $=0.1 \\times 10^{-2}$\n",
        "\n",
        "  * **Concrete**: learning rate $= 0.7 \\times 10^{-3}$, tolerance $ = 0.1 \\times 10^{-3}$.\n",
        "\n",
        "\n",
        "Note: Here tolerance is defined based on the differenmt in root mean squared error (RMSE) measured on the training set between successive iterations. Where, RMSE is defined as \n",
        "\n",
        "\\begin{align}\n",
        "RMSE = \\sqrt{\\frac{SSE}{N}}\n",
        "\\end{align}\n",
        "\n",
        "$N$ is the number of training instances.\n",
        "\n",
        "4. Be sure to set a maximum number of interations (recommended maximum iterations = 50000) so that the algorithm does not run forever."
      ],
      "metadata": {
        "id": "SHQh_YTsXPi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Normalization**:\n",
        "\n",
        "In the previous assignment we normalized the entire dataset prior to learning, and then used ten-fold cross validation to evaluate the performance of the learning algorithm. However, the correct way of normalizing data would be to normalize the training data and record the normalize paraters (i.e. mean and standard deviation) for $z$-score normalization and min and max feature values for feature rescaling. This minimizes the chances of introducing any bias buring the performance evaluation of the learning algorithm. In  a real work scenario you would not have access to test data during the training phase.\n",
        "\n",
        "To **summarize**: only nomralize your training data and then use nomalization parameters to normalize your test data and then estimate the accuracy/error.\n",
        "\n",
        "**Adding the Constant Feature**: For every regression problem remember to add a column of ones to your dataset."
      ],
      "metadata": {
        "id": "u3EHBMCNbVhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Least Regression using Normal Equations**\n",
        "\n",
        "Use the `housing` and `yacht` data set to estimate the regression using normal equation. Contrast the performance (measured through RMSE) to the results obtained using gradient descent algorithm. In this problem, you will calculate the analytic solution that we obtained through Noraml Equations to learn your weight vector, and constrast the performance (training and test RMSE) for your graident-descent based impmentation for problem-1."
      ],
      "metadata": {
        "id": "CVwaw0LvdLVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Students should add the following functions to their model and verify the results:\n",
        "\n",
        "1. Add **regularization** term using $\\|\\vec{\\theta}\\|_{2}^{2}$ to:\n",
        "  * **Normal equation** (closed form solution) [5 points]\n",
        "  * **Gradient Descent** [5 points]\n",
        "  * Be sure to pass the **regularization** as a **parameter** to your **class**, so you could run your model **with** regularization or **without** regularization term (version we saw in class)\n",
        "\n",
        "2. Add a function to calculate the **Stochastic gradient descent** [10 points]\n",
        "  * Be sure to pass the necessary **parameters** to your class so you could switch between **Gradient descent** or **stochastic gradient descent**.\n",
        "\n",
        "3. Add a plotting function that will plot the error costs of the gradient descent. \n",
        "\n",
        "    * Be sure to plot the cost of your model during the training steps.\n",
        "\n",
        "Report the RMSE and SSE over the test set for all three datasets.\n",
        "\n",
        "Describe your observation and what are the effects of the learning rate and regularization parameters on the learning."
      ],
      "metadata": {
        "id": "8IEhewPgeiEZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqSKmcHCpXjG"
      },
      "outputs": [],
      "source": [
        "# import Libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression:\n",
        "  def __init__(self, X, y, learningRate, Error, sample_size, \n",
        "               epsilon, maxIteration, lam, sgd = False, gd = False, reg = False) -> None:\n",
        "    self.X = X # The default value for X is to let user input the prediction data \n",
        "    self.y = y # The default value for y is to let user input the target variable\n",
        "    self.learningRate = learningRate # The default value for learning rate is to let user input the learning rate for gradient descient\n",
        "    self.Error = Error # The input value for error is to let user descide wich type of error they want to input \n",
        "    self.sample_size = sample_size # The default value for sample_size is to let user decide the sample size for stochastic gradient \n",
        "    self.epsilon = epsilon # the default value for epsilon is to let user input the tolarence for gradient descient \n",
        "    self.maxIteration = maxIteration # The default value for maximum iteration for user input\n",
        "    self.lam = lam # Let user input regularized parameter \n",
        "    self.sgd = sgd # let user input if they want to use the stochastic gradient; default is false \n",
        "    self.gd = gd # let user input if they want to use the gradient descent; default is false\n",
        "    self.reg = reg # Let user input if they want to use the regularization; defualt is false \n",
        "\n",
        "\n",
        "  ##Define a function to split original data into test portion and training portion\n",
        "  def splitData(self):\n",
        "    X_train, X_test, Y_test, Y_train = train_test_split(self.X, self.y, \n",
        "                                                        test_size = 0.3, \n",
        "                                                        random_state = 0)\n",
        "    return X_train, X_test, Y_test, Y_train\n",
        "\n",
        "  ## Define a function to add a 0 column to the data as coefficient for linear regression\n",
        "  def addX0(self, X):\n",
        "    return np.column_stack([np.ones(X.shape[0]), X])\n",
        "\n",
        "  ## Define a function to normalize the training data \n",
        "  def normalizeData(self, X):\n",
        "    mean = np.mean(X, 0) # find the mean of each variable\n",
        "    std = np.std(X,0) # find the standard deviation of each variable \n",
        "    X_norm = (X - mean) / std # using nomalization formula to normalize the data\n",
        "    X_norm = self.addX0(X_norm) # adding 0 coulumn after normalize the data\n",
        "    return X_norm, mean, std\n",
        "  \n",
        "  ## Define a function to normalize the training data set using the scalar define for training data set\n",
        "  def normalizeTestData(self, X, mean, std):\n",
        "    X_norm = (X - mean) / std # normalize the data\n",
        "    X_norm = self.addX0(X_norm)  # adding zero column after normalize the data\n",
        "    return X_norm\n",
        "\n",
        "  ## Define a function to check if the data if full rank\n",
        "  def checkFullRank(self, X):\n",
        "    rank = np.linalg.matrix_rank(X) # Find the rank of the data \n",
        "    if rank == min(X.shape[0], X.shape[1]): # the rank should be equal to the min(#number of rows of the data, number of columns of the data)\n",
        "      print('Data is Full Rank') # If it is true, print the data is full rank \n",
        "      self.Fullrank = True\n",
        "    else: # other wise\n",
        "      print('Data is Not Full Rank') # print the data is not full rank \n",
        "      self.Fullrank = False\n",
        "\n",
        "  ## Define a function to check if the data is low rank\n",
        "  def checkLowRank(self, X):\n",
        "    if X.shape[0] < X.shape[1]: # if the number of rows is less than  then number of columns \n",
        "      print('Data is Low Rank') # print the data is low rank \n",
        "      self.lowRank = True\n",
        "    else: # other wise\n",
        "      print('Data is not Low Rank') # The data is not low rank \n",
        "      self.lowRank = False\n",
        "\n",
        "  ## Define a function for normal equation for basic linear regression\n",
        "  def normalEquation(self, X, y):\n",
        "    w = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "    return w\n",
        "\n",
        "  ## Define a function to predict the target variable\n",
        "  def predict(self, X):\n",
        "    y_hat = X.dot(self.w)\n",
        "    return y_hat\n",
        "\n",
        "  ## Define a function for normal equation for regularized linear regression \n",
        "  def normalReg(self, X, y):\n",
        "    X_dim = self.addX0(X) # adding 0 column to the data set \n",
        "    dim = X_dim.shape[1] # find the dimension of the data set \n",
        "    I = np.identity(dim - 1) # Determine the identity matrix\n",
        "    w = np.linalg.inv((X.T).dot(X) + self.lam * I).dot(X.T).dot(y)\n",
        "    return w\n",
        "\n",
        "  ## Define a function to calculate the sum squared error \n",
        "  def sse(self, X, y):\n",
        "      y_hat = self.predict(X)\n",
        "      return ((y_hat - y)**2).sum()\n",
        "\n",
        "  ## Define a function to calculate the cost function \n",
        "  def costFunction(self, X, y):\n",
        "      return self.sse(X, y) / 2 # Which is sse /2 (dividing 2 here is for easier calclulation)\n",
        "\n",
        "  # Define a function for cost derivative\n",
        "  def costDerivative(self, X, y):\n",
        "      return X.T.dot(self.predict(X) - y)\n",
        "\n",
        "  # Define a function for root mean squared error \n",
        "  def rmse(self, X, y):\n",
        "    return math.sqrt(self.sse(X, y)/y.size)\n",
        "\n",
        "  ## Define a function for gradient descent for basic linear regression \n",
        "  def gradientDescent(self, X, y):\n",
        "    errors = [] # Define an empty list for collecting the errors\n",
        "    prev_error = float('inf')  # suppose that previous error is very large \n",
        "    self.costValues = [] # define a function to collecte the cost values \n",
        "    for i in tqdm(range(self.maxIteration)): # loop the iteration from using tqdm function \n",
        "      self.w = self.w - self.learningRate * self.costDerivative(X, y) # use the gradient descent method where costDerivative is the gradient \n",
        "      cost = self.costFunction(X, y) # The cost is the costFunction for X and y\n",
        "      self.costValues.append(cost) # update the costValue list \n",
        "      if self.Error == 'rmse': # If the user wants to report rmse\n",
        "        current_error = self.rmse(X, y) # find the rmse\n",
        "      else: # other wise\n",
        "        current_error = self.sse(X, y) # go for sse\n",
        "      diff = current_error - prev_error # the error is defind as current error - prev error\n",
        "      errors.append(current_error) # update the error list with current error \n",
        "      prev_error = current_error # swap prev error and current error \n",
        "      if abs(diff) < self.epsilon: # if the abs or current error - prev error is less than the desired tolarence\n",
        "        print('Model is stopped learning.') # The model is stopped learning \n",
        "        break\n",
        "    self.plot_rmse(errors)\n",
        "\n",
        "  # Define a function for calculate the cost derivative for regularized linear regression \n",
        "  def costDerivativeReg(self, X, y):\n",
        "    y_hat = self.predict(X)\n",
        "    return (y_hat - y).dot(X) + self.lam*(self.w)\n",
        "\n",
        "  # Define a gradient descent method for regularized linear regression \n",
        "  def gdReg(self, X, y):\n",
        "    errors = [] # Define an empty list for collecting the errors \n",
        "    self.costValues = [] # Define an empty list for colletecting the cost valus \n",
        "    prev_error  = float('inf') # suppse the previous error is very big \n",
        "    for i in tqdm(range(self.maxIteration)): # loop the iteration using tqdm function \n",
        "      self.w = self.w - self.learningRate*self.costDerivativeReg(X, y) # use the graident descient method for regularized linear regression \n",
        "      cost = self.costFunction(X, y) # The cost value is the cost function \n",
        "      self.costValues.append(cost) # update the cost value\n",
        "      if self.Error == 'rmse': # If the user wants to repost the rmse \n",
        "        current_error  = self.rmse(X, y) # current error is the rmse\n",
        "      else: # otherwise\n",
        "        current_error = self.sse(X, y) # the current error is sse\n",
        "      diff = current_error - prev_error # The error is current error - prev error \n",
        "      prev_error = current_error    # swap prev and current error \n",
        "      errors.append(current_error) # update the error list \n",
        "      if abs(diff) < self.epsilon: # If the abs(current and prev errors) < desired tol\n",
        "        print('Model is stopped learning') # model is stopped learning\n",
        "        break\n",
        "    self.plot_rmse(errors)\n",
        "\n",
        "  # Deinfe a function for stochastic gradient\n",
        "  def stochGD(self, X, y):\n",
        "    errors = [] # Define an empty list to collect the errors \n",
        "    self.costValues = [] # Define an empty list for cost values\n",
        "    prev_error = float('inf') # Suppse that prev error is very large\n",
        "    for i in tqdm(range(self.maxIteration)): # Loop the iteration using tqdm function \n",
        "      b = random.sample(list(X), self.sample_size) # Find the sample size for X \n",
        "      c = random.sample(list(y), self.sample_size) # Find the sample size for y\n",
        "      y = np.array(c) # Transfer the y to a vector\n",
        "      X = np.array(b) # transfer the X to a matrix\n",
        "      for q,r in zip(X, y): # Define the set of points for (X,y)\n",
        "        self.w = self.w - self.learningRate*self.costDerivative(X, y) # Using the gradient descent for basic linear regression \n",
        "        cost = self.costFunction(X, y) # Find the cost values\n",
        "        self.costValues.append(cost) # update the cost values \n",
        "        if self.Error == 'rmse': # If the user wants to report the rmse\n",
        "          current_error = self.rmse(X, y) # The current error is rmse\n",
        "        else: #otherwise\n",
        "          current_error = self.sse(X, y) # The current error is sse\n",
        "        diff = current_error - prev_error # the error is current_error - prev_error \n",
        "        prev_error = current_error # swap prev and current errors      \n",
        "        errors.append(current_error) # update the error list\n",
        "        if abs(diff) < self.epsilon: # if the abs(error) < desired tol \n",
        "          #print('Model is stopped learning') #Model is topped learning \n",
        "          break      \n",
        "    self.plot_rmse(errors)\n",
        "\n",
        "  # Define a function for stochastic gradicent for regularized linear regression \n",
        "  def stochGdReg(self, X, y):\n",
        "    errors= [] # Define an empty list for errors \n",
        "    self.costValues = [] # Define an empty list for cost values \n",
        "    prev_error = float('inf')  # Suppse that prev error is very large\n",
        "    for i in tqdm(range(self.maxIteration)): # Loop the iteration using tqdm function\n",
        "      b = random.sample(list(X), self.sample_size) # Find the sample size for X \n",
        "      c = random.sample(list(y), self.sample_size) # Find the sample size for y\n",
        "      y = np.array(c) # Transfer the y to a vector\n",
        "      X = np.array(b) # transfer the X to a matrix\n",
        "      for q, r in zip(X, y): # Define the set of points for (X,y)\n",
        "        self.w = self.w - self.learningRate*self.costDerivativeReg(X,y) # use the graident descient method for regularized linear regression\n",
        "        cost = self.costFunction(X, y) # update the cost values\n",
        "        self.costValues.append(cost) # update the cost values \n",
        "        if self.Error == 'rmse': #If the user wants to report the rmse\n",
        "          current_error = self.rmse(X, y) # The current error is rmse\n",
        "        else: #otherwise\n",
        "          current_error = self.sse(X,y)     # The current error is sse \n",
        "        diff = current_error - prev_error # the error is current_error - prev_error\n",
        "        prev_error = current_error     # Swap the previous error and current error \n",
        "        errors.append(current_error)    # Update the error lisr with current errors        \n",
        "        if abs(diff) < self.epsilon: # If the abs(current error - prev error) < a desired tol \n",
        "          #print('Model is stopped learning.') # Model will stop learning\n",
        "          break\n",
        "      plt.plot(errors)\n",
        "\n",
        "  # Define a function to plot the room mean sqaured error \n",
        "  def plot_rmse(self, error_sequence):\n",
        "    # Data for plotting\n",
        "    s = np.array(error_sequence)\n",
        "    t = np.arange(s.size)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(t, s)\n",
        "\n",
        "    ax.set(xlabel='Iterations', ylabel = 'Error',\n",
        "           title='Iterations andError Trend')\n",
        "    ax.grid()\n",
        "    plt.legend(bbox_to_anchor=(1.05,1), loc=2, shadow=True)\n",
        "    plt.show()\n",
        "  \n",
        "  # Define a function for fit \n",
        "  def fit(self):\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = self.splitData() # Data spliting \n",
        "    self.X_train, self.mean, self.sd = self.normalizeData(self.X_train) # Normalize the  traing data \n",
        "    self.X_test = self.normalizeTestData(self.X_test, self.mean, self.sd) # Normalize the test data\n",
        "    self.checkFullRank(self.X_train) # Check the full rank \n",
        "    self.checkLowRank(self.X_train)  # Check the low rank \n",
        "\n",
        "    # If the data is full rank it is not no rank . If the gradience descent, regularization, and stochastic gradient descent\n",
        "      ## are not going to be used  \n",
        "    if self.Fullrank and not self.lowRank and not self.gd and not self.reg and not self.sgd: \n",
        "      # go for closed form solution for basic linear regression   \n",
        "      print('Closed Form Solution for Basic Linear Regression')\n",
        "      self.w = self.normalEquation(self.X_train, self.y_train)\n",
        "\n",
        "    # if the data is full rank, not low rank, gradient descent is not going to be used, stochatic gradient is not going to be used\n",
        "      ## regularization is going to be used           \n",
        "    elif self.Fullrank and not self.lowRank and not self.gd and self.reg and not self.sgd:\n",
        "      # go for closed form solution for regularizaed linear regression\n",
        "      print('Closed Form Solution Regularization Linear Regression')\n",
        "      self.w = self.normalReg(self.X_train, self.y_train)\n",
        "\n",
        "    # If the gradient is going to be used and regularization is going to be used and stochastic gradient descent is not going to be used:\n",
        "    elif self.gd and self.reg and not self.sgd:\n",
        "      # go for gradient descent for regularized linear regression\n",
        "      print('Gradient Descent for Regularization Linear Regression')\n",
        "      #Initialize your weights\n",
        "      self.w = np.zeros(self.X_train.shape[1]) #initialize weights to zeros\n",
        "      self.gdReg(self.X_train, self.y_train)\n",
        "    \n",
        "    # If the stochasich gradient is going to be used, if the regularization is going to be used\n",
        "      ## if the graident is not going to be used\n",
        "    elif self.sgd and self.reg and not self.gd:\n",
        "      # go for stochastic gradient descent for regularized linear regression\n",
        "      print('Stochastic Gradient Descent for Regularization Linear Regression')\n",
        "      # Initialize your weights\n",
        "      self.w = np.zeros(self.X_train.shape[1]) #initialize weights to zeros\n",
        "      self.stochGdReg(self.X_train, self.y_train)\n",
        "    \n",
        "    # If stochastic gradient is going to be used, regularization is not going to be used, as well as gradient descient is not going to be used\n",
        "    elif self.sgd and not self.reg and not self.gd:\n",
        "      # Go for stochastic gradient descent \n",
        "      print('Stochastic Gradient Descent')\n",
        "      # Initialize your starting weights\n",
        "      self.w = np.zeros(self.X_train.shape[1]) # Initialize weights to zeors\n",
        "      self.stochGD(self.X_train, self.y_train)\n",
        "\n",
        "    # Else\n",
        "    else:\n",
        "      # Use the gradient descent for basic linear regression\n",
        "      print(\"Gradient Descent for Basic Linear Regression\")\n",
        "      # Initialize your weights\n",
        "      self.w = np.zeros(self.X_train.shape[1]) # Initialize weights to all zeors \n",
        "      self.gradientDescent(self.X_train,self.y_train)\n",
        "\n",
        "    sse_test = self.sse(self.X_test, self.y_test) # Find the sum of squared error on testing data\n",
        "    rmse_test = self.rmse(self.X_test, self.y_test) # Find the root mean squared error on testing data\n",
        "    sse_train = self.sse(self.X_train, self.y_train) # Find the sum of squared error on training data \n",
        "    rmse_train = self.rmse(self.X_train, self.y_train) # Find the sse on training data\n",
        "    print(\"The SSE for test data is {0}\". format(round(sse_test,2))) # Primt the SSE and round your result to 2 decimals \n",
        "    print(\"The RMSE for test data is {0}\". format(round(rmse_test,2))) # Print the RMSE and round your result to 2 decimals\n",
        "    print(\"The SSE for training data is {0}\". format(round(sse_train,2)))\n",
        "    print(\"The RMSE for training data is {0}\". format(round(rmse_train,2)))\n",
        "    print(\"Weights:\") \n",
        "    print(self.w) # Print the weight "
      ],
      "metadata": {
        "id": "7dMO7ajbpom4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Housing data**: \n",
        "\n",
        "This is a regression dataset where the task is to predict the value of houses in the suburbs of Boston based on thirteen features that describe different aspects that are relevent to determining the value of a house, such as the number of rooms, levels of population in the area, etc."
      ],
      "metadata": {
        "id": "1dz7see_V6-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the housing data set\n",
        "df1 = pd.read_csv(\"Project Dataset.csv\")\n",
        "df1"
      ],
      "metadata": {
        "id": "5Wyr2Qc311Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1['Runtime']=df1[['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)']].mean(axis=1)\n",
        "df1=df1.drop(columns =['Run1 (ms)','Run2 (ms)','Run3 (ms)','Run4 (ms)'], axis = 1)"
      ],
      "metadata": {
        "id": "T8AzHdfW_Vpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "jW2Wk4bL_YyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df1.values[:,0:-1]"
      ],
      "metadata": {
        "id": "wvJ3bucFtYPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df1.values[:,-4]"
      ],
      "metadata": {
        "id": "2KIZqPPmtUbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Housing data"
      ],
      "metadata": {
        "id": "T774GKPMJQAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the normal equation on Housing data set without regularization\n",
        "clf1 = LinearRegression(df1.values[:,0:-1], \n",
        "                              df1.values[:,-1], sample_size = 100, lam = 0.001, learningRate = 0.0004, Error = 'rmse',\n",
        "                        epsilon = 0.005, maxIteration = 500, sgd=False, gd = False, reg = False)\n",
        "clf1.fit()"
      ],
      "metadata": {
        "id": "0caFuSI21_m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the gradient descent on Housing data set without regularization\n",
        "clf2 = LinearRegression(df1.values[:,0:-1], \n",
        "                              df1.values[:,-1], sample_size = 100, lam = 0.0001, learningRate = 0.0004, Error = 'rmse',\n",
        "                        epsilon = 0.005, maxIteration = 50000, sgd=False, gd = False, reg = True)\n",
        "clf2.fit()"
      ],
      "metadata": {
        "id": "ov1AUxG2h57K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the basic gradient descent on Housing data set without regularization\n",
        "clf3 = LinearRegression(df1.values[:,0:-1], \n",
        "                              df1.values[:,-1], sample_size = 100, lam = 0.0001, learningRate = 0.0004, Error = 'rmse',\n",
        "                        epsilon = 0.005, maxIteration =500, sgd=False, gd = True, reg = False)\n",
        "clf3.fit()"
      ],
      "metadata": {
        "id": "mTiDYS86IlRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the regularized gradient descent on Housing data set without regularization\n",
        "clf4 = LinearRegression(df1.values[:,0:-1], \n",
        "                              df1.values[:,-1], sample_size = 100, lam = 0.0001, learningRate = 0.0004, Error = 'rmse',\n",
        "                        epsilon = 0.005, maxIteration = 500, sgd=False, gd = True, reg = True)\n",
        "clf4.fit()"
      ],
      "metadata": {
        "id": "d3jiYq81JNqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the stochastic gradient descent on Housing data set \n",
        "clf5 = LinearRegression(df1.values[:,0:-1], \n",
        "                              df1.values[:,-1], sample_size = 200, lam = 0.0001, learningRate = 0.0004, Error = 'rmse',\n",
        "                        epsilon = 0.005, maxIteration = 50, sgd=True, gd = False, reg = False)\n",
        "clf5.fit()"
      ],
      "metadata": {
        "id": "J7a4f9G8UfGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine the root mean squared error for housing data used by normal equation\n",
        "RMSE_Housing = clf1.rmse(clf1.X_test, clf1.y_test)\n",
        "print(\"The RMSE for hosing data on test portion is {0}\".format(RMSE_Housing))"
      ],
      "metadata": {
        "id": "JoYy2FUbKbAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cNX4BHLckWhN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}